{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-30T06:59:02.750920Z",
     "iopub.status.busy": "2022-12-30T06:59:02.750294Z",
     "iopub.status.idle": "2022-12-30T07:01:03.041776Z",
     "shell.execute_reply": "2022-12-30T07:01:03.040658Z",
     "shell.execute_reply.started": "2022-12-30T06:59:02.750879Z"
    },
    "tags": []
   },
   "source": [
    "# 1：下载没有的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T03:16:44.192018Z",
     "iopub.status.busy": "2022-12-31T03:16:44.190995Z",
     "iopub.status.idle": "2022-12-31T03:16:44.195404Z",
     "shell.execute_reply": "2022-12-31T03:16:44.194699Z",
     "shell.execute_reply.started": "2022-12-31T03:16:44.191983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!mkdir /home/aistudio/external-libraries\n",
    "#!pip install visualdl -t /home/aistudio/external-libraries\n",
    "#!pip install d2l -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2：导入所需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T11:24:31.427096Z",
     "iopub.status.busy": "2023-01-11T11:24:31.426576Z",
     "iopub.status.idle": "2023-01-11T11:24:34.213251Z",
     "shell.execute_reply": "2023-01-11T11:24:34.212203Z",
     "shell.execute_reply.started": "2023-01-11T11:24:31.427066Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "%matplotlib inline\n",
    "import os\n",
    "import paddle\n",
    "import paddle.vision\n",
    "from paddle.nn import functional as F\n",
    "from paddle import nn\n",
    "from d2l import paddle as d2l\n",
    "from paddle.vision.datasets.folder import default_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1：加入可视化模块，记得改名字！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T11:24:38.107862Z",
     "iopub.status.busy": "2023-01-11T11:24:38.107343Z",
     "iopub.status.idle": "2023-01-11T11:24:38.221037Z",
     "shell.execute_reply": "2023-01-11T11:24:38.219909Z",
     "shell.execute_reply.started": "2023-01-11T11:24:38.107827Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入VisualDL的包\n",
    "from visualdl import LogWriter\n",
    "logw = LogWriter(\"./random_log/log—1\", sync_cycle=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3：文件解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T03:16:46.779034Z",
     "iopub.status.busy": "2022-12-31T03:16:46.778560Z",
     "iopub.status.idle": "2022-12-31T03:16:46.784060Z",
     "shell.execute_reply": "2022-12-31T03:16:46.783447Z",
     "shell.execute_reply.started": "2022-12-31T03:16:46.779007Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def un_zip(file_name, dst):\n",
    "    \"\"\"解压 zip 文件\"\"\"\n",
    "    zip_file = zipfile.ZipFile(file_name)\n",
    "    if os.path.isdir(dst):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(dst)\n",
    "    for names in zip_file.namelist():\n",
    "        zip_file.extract(names, dst)\n",
    "    zip_file.close()\n",
    "\n",
    "\n",
    "\n",
    "file_name = r\"/home/aistudio/data/data183509/release_data.zip\"\n",
    "dst = r\"./work/\"\n",
    "#un_zip(file_name, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4：定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T11:24:45.421446Z",
     "iopub.status.busy": "2023-01-11T11:24:45.420851Z",
     "iopub.status.idle": "2023-01-11T11:24:45.431699Z",
     "shell.execute_reply": "2023-01-11T11:24:45.430780Z",
     "shell.execute_reply.started": "2023-01-11T11:24:45.421414Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class readImageDataset(paddle.io.Dataset):\n",
    "    def __init__(self, state, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.state = state\n",
    "        self.root = r\"./data/release_data\"  # 该文件夹内包含train、val以及test三个文件夹\n",
    "        self.labels, self.paths = self.getDataPath()\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    # 定义一个读取数据集的子类\n",
    "    def getDataPath(self):\n",
    "        paths = []\n",
    "        labels = []\n",
    "        assert self.state == 'train' or self.state == 'val' or self.state == 'test'  # 用于选取所读指定的数据集\n",
    "        if self.state == 'train':\n",
    "            f_path = r'./work/release_data/train_list.txt'#train模式文件地址\n",
    "            path_compose=r'train'\n",
    "\n",
    "        if self.state == 'val':\n",
    "            f_path = r'./work/release_data/val_list.txt'\n",
    "            path_compose = r'val'\n",
    "\n",
    "        if self.state == 'test':\n",
    "            f_path = r'./work/release_data/test_truth_list.txt'\n",
    "            path_compose = r'test'\n",
    "\n",
    "        with open(f_path) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                if \"006626.jpg\" not in line:\n",
    "                   linestr = line.split(' ')  # 一行有多个数据，以空格为间隔将字符串隔开读取\n",
    "                   paths.append(os.path.join(self.root,path_compose, linestr[0]))\n",
    "                         # 读取该行的第一个数据\n",
    "                   labels.append(int(linestr[1]))\n",
    "                line = f.readline()  # 从下一行读取一个新的值出来\n",
    "        return (labels, paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = default_loader(path)\n",
    "        if self.target_transform is not None:\n",
    "            try:\n",
    "                image = self.target_transform(image)\n",
    "            except IOError:\n",
    "                print(\"多余的地址：\",path)\n",
    "                self.__getitem__(10)\n",
    "        return image,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5：图像预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T11:24:48.733279Z",
     "iopub.status.busy": "2023-01-11T11:24:48.732187Z",
     "iopub.status.idle": "2023-01-11T11:24:49.295871Z",
     "shell.execute_reply": "2023-01-11T11:24:49.294907Z",
     "shell.execute_reply.started": "2023-01-11T11:24:48.733244Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize = paddle.vision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = paddle.vision.transforms.Compose([\n",
    "    paddle.vision.transforms.RandomResizedCrop(380),\n",
    "    paddle.vision.transforms.RandomHorizontalFlip(),\n",
    "    paddle.vision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "test_augs = paddle.vision.transforms.Compose([\n",
    "    paddle.vision.transforms.Resize(420),\n",
    "    paddle.vision.transforms.CenterCrop(380),\n",
    "    paddle.vision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "To_Tensor=paddle.vision.transforms.ToTensor()\n",
    "transform=paddle.vision.transforms.Compose([To_Tensor,paddle.vision.transforms.Resize(96)])\n",
    "train_datas=readImageDataset('train',target_transform=train_augs)\n",
    "test_datas=readImageDataset('test',target_transform=test_augs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T02:52:19.483342Z",
     "iopub.status.busy": "2022-12-31T02:52:19.482837Z",
     "iopub.status.idle": "2022-12-31T02:52:19.487502Z",
     "shell.execute_reply": "2022-12-31T02:52:19.486729Z",
     "shell.execute_reply.started": "2022-12-31T02:52:19.483310Z"
    },
    "tags": []
   },
   "source": [
    "# 5：加载预训练网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T03:18:37.447609Z",
     "iopub.status.busy": "2022-12-31T03:18:37.447080Z",
     "iopub.status.idle": "2022-12-31T03:18:40.887457Z",
     "shell.execute_reply": "2022-12-31T03:18:40.886683Z",
     "shell.execute_reply.started": "2022-12-31T03:18:37.447575Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\r\n",
      "  (conv1): Conv2D(3, 64, kernel_size=[7, 7], stride=[2, 2], padding=3, data_format=NCHW)\r\n",
      "  (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\r\n",
      "  (relu): ReLU()\r\n",
      "  (maxpool): MaxPool2D(kernel_size=3, stride=2, padding=1)\r\n",
      "  (layer1): Sequential(\r\n",
      "    (0): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(64, 128, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(128, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "      (downsample): Sequential(\r\n",
      "        (0): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "        (1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (1): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(256, 128, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(128, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (2): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(256, 128, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(128, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (layer2): Sequential(\r\n",
      "    (0): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(256, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(256, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "      (downsample): Sequential(\r\n",
      "        (0): Conv2D(256, 512, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\r\n",
      "        (1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (1): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(512, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(256, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (2): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(512, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(256, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (3): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(512, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(256, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (layer3): Sequential(\r\n",
      "    (0): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(512, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "      (downsample): Sequential(\r\n",
      "        (0): Conv2D(512, 1024, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\r\n",
      "        (1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (1): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(512, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (2): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(512, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (3): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(512, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (4): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(512, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (5): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(512, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (layer4): Sequential(\r\n",
      "    (0): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(1024, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "      (downsample): Sequential(\r\n",
      "        (0): Conv2D(1024, 2048, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\r\n",
      "        (1): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (1): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(2048, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(1024, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "    (2): BottleneckBlock(\r\n",
      "      (conv1): Conv2D(2048, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, data_format=NCHW)\r\n",
      "      (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "      (conv3): Conv2D(1024, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "      (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "      (relu): ReLU()\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (avgpool): AdaptiveAvgPool2D(output_size=(1, 1))\r\n",
      "  (fc): Sequential(\r\n",
      "    (0): Linear(in_features=2048, out_features=300, dtype=float32)\r\n",
      "    (1): ReLU()\r\n",
      "    (2): Dropout(p=0.4, axis=None, mode=upscale_in_train)\r\n",
      "    (3): Linear(in_features=300, out_features=208, dtype=float32)\r\n",
      "  )\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "finetune_net = paddle.vision.models.wide_resnet50_2(pretrained=True)\n",
    "finetune_net.fc = nn.Sequential(nn.Linear(finetune_net.fc.state_dict()['weight'].shape[0], 300),nn.ReLU(),nn.Dropout(0.4),nn.Linear(300,208))\n",
    "for layer in finetune_net.fc:\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.initializer.XavierUniform(layer.state_dict()['weight'])\n",
    "print(finetune_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0：新版加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T11:41:30.338726Z",
     "iopub.status.busy": "2023-01-11T11:41:30.337814Z",
     "iopub.status.idle": "2023-01-11T11:41:31.120626Z",
     "shell.execute_reply": "2023-01-11T11:41:31.119643Z",
     "shell.execute_reply.started": "2023-01-11T11:41:30.338689Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=paddle.jit.load(\"./work/inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T07:34:41.899728Z",
     "iopub.status.busy": "2023-01-11T07:34:41.899173Z",
     "iopub.status.idle": "2023-01-11T07:34:41.903859Z",
     "shell.execute_reply": "2023-01-11T07:34:41.903150Z",
     "shell.execute_reply.started": "2023-01-11T07:34:41.899690Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TranslatedLayer.program of TranslatedLayer()>\r\n"
     ]
    }
   ],
   "source": [
    "    print(model.program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1是否用自己的参数，记得更改加载的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-31T03:17:00.475697Z",
     "iopub.status.idle": "2022-12-31T03:17:00.476535Z",
     "shell.execute_reply": "2022-12-31T03:17:00.476363Z",
     "shell.execute_reply.started": "2022-12-31T03:17:00.476347Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finetune_net.set_state_dict(paddle.load('net_dict_6.pdparams'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6：训练前定义训练的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-31T03:17:00.477737Z",
     "iopub.status.idle": "2022-12-31T03:17:00.478041Z",
     "shell.execute_reply": "2022-12-31T03:17:00.477904Z",
     "shell.execute_reply.started": "2022-12-31T03:17:00.477891Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus(),records=None):\n",
    "    \"\"\"Defined in :numref:`sec_image_augmentation`\"\"\"\n",
    "    \"\"\"用多GPU进行模型训练\n",
    "    Defined in :numref:`sec_image_augmentation`\"\"\"\n",
    "    step=0\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    net = paddle.DataParallel(net)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 4个维度：储存训练损失，训练准确度，实例数，特点数\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            step=step+1\n",
    "            if (i + 1) % (num_batches // 10) == 0 or i == num_batches - 1:\n",
    "                logw.add_scalar('loss',metric[0] / metric[2], step)\n",
    "                logw.add_scalar('train_acc',metric[1] / metric[3], step)\n",
    "                print(epoch + (i + 1) / num_batches)   \n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        logw.add_scalar('test_acc', test_acc, step)\n",
    "        print('epoc:', epoch + (i + 1) / num_batches, 'loss:', metric[0] / metric[2], 'train_acc:',\n",
    "              metric[1] / metric[3],'test_acc', test_acc)\n",
    "        print(\"\\ntime:\",datetime.datetime.now())\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-31T03:17:00.478973Z",
     "iopub.status.idle": "2022-12-31T03:17:00.479264Z",
     "shell.execute_reply": "2022-12-31T03:17:00.479136Z",
     "shell.execute_reply.started": "2022-12-31T03:17:00.479123Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=25,\n",
    "                      param_group=True):\n",
    "    train_iter = paddle.io.DataLoader(train_datas,\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    test_iter = paddle.io.DataLoader(test_datas,\n",
    "        batch_size=batch_size)\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"fc.0.weight\", \"fc.0.bias\",\"fc.3.weight\", \"fc.3.bias\"]]\n",
    "        trainer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=[{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'learning_rate': learning_rate*10}],\n",
    "                                    weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=net.parameters(), \n",
    "                                  weight_decay=0.001)\n",
    "    train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,devices,records=logw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7：开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-31T03:17:00.480720Z",
     "iopub.status.idle": "2022-12-31T03:17:00.481013Z",
     "shell.execute_reply": "2022-12-31T03:17:00.480882Z",
     "shell.execute_reply.started": "2022-12-31T03:17:00.480869Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_fine_tuning(finetune_net, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8：保存模型，记得改名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-31T03:17:00.481867Z",
     "iopub.status.idle": "2022-12-31T03:17:00.482206Z",
     "shell.execute_reply": "2022-12-31T03:17:00.482035Z",
     "shell.execute_reply.started": "2022-12-31T03:17:00.482021Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "paddle.save(finetune_net.state_dict(), 'net_dict_one.pdparams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9：预测函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 把标签变成人话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T11:41:16.904583Z",
     "iopub.status.busy": "2023-01-11T11:41:16.903227Z",
     "iopub.status.idle": "2023-01-11T11:41:17.294255Z",
     "shell.execute_reply": "2023-01-11T11:41:17.293287Z",
     "shell.execute_reply.started": "2023-01-11T11:41:16.904524Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "def xlsx_trans(root):\n",
    "    mybook=openpyxl.load_workbook(root,data_only=True)\n",
    "    mySheet=mybook.active\n",
    "    #按行获取新书表的单元格（第一行除外--标题，不是数据）\n",
    "    myRows=list(mySheet.values)[:]\n",
    "    mydics={}\n",
    "    for myRow in myRows:\n",
    "        mydics[myRow[0]]=myRow[1]\n",
    "    return mydics\n",
    "mydicts=xlsx_trans(\"/home/aistudio/data/data183509/class_names.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 定义预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:46:45.878899Z",
     "iopub.status.busy": "2023-01-11T12:46:45.878014Z",
     "iopub.status.idle": "2023-01-11T12:46:45.890846Z",
     "shell.execute_reply": "2023-01-11T12:46:45.889817Z",
     "shell.execute_reply.started": "2023-01-11T12:46:45.878845Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(path,transform,net,label_read,top_x):\n",
    "    list=[]\n",
    "    list1=[]\n",
    "    img=default_loader(path)\n",
    "    img_tensor=transform(img)\n",
    "    img_tensor=paddle.unsqueeze(img_tensor,axis=0)\n",
    "    net.eval()\n",
    "    x=net(img_tensor)\n",
    "    xl=(x.numpy().tolist())[0]\n",
    "    temp={}\n",
    "    for i in range(top_x):\n",
    "      temp[mydicts[int(xl.index(max(xl)))]] = max(xl)\n",
    "      # list.append(xl.index(max(xl)))\n",
    "      xl[xl.index(max(xl))]=0\n",
    "    # for i in list:\n",
    "    #     list1.append(mydicts[int(i)])\n",
    "    # if top_x == 1:\n",
    "    return temp\n",
    "    # else:\n",
    "    #   return list1\n",
    "\n",
    "\n",
    "def predict_list(path_list,transform,net,label_read,top_x):\n",
    "  list=os.listdir(path_list)\n",
    "  result=[]\n",
    "  result1=[]\n",
    "  for i in list:\n",
    "    if \"jpg\" in i:\n",
    "      path=os.path.join(path_list,i)\n",
    "      result.append(predict(path,transform,net,label_read,top_x))\n",
    "      result1.append(i)\n",
    "  return (result,result1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件夹整体预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T12:58:48.298961Z",
     "iopub.status.busy": "2023-01-11T12:58:48.298244Z",
     "iopub.status.idle": "2023-01-11T12:58:54.983966Z",
     "shell.execute_reply": "2023-01-11T12:58:54.982927Z",
     "shell.execute_reply.started": "2023-01-11T12:58:48.298908Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'凉拌凉面': 0.5653793811798096, '茄汁拌面': 0.08540446311235428, '炒面': 0.07997732609510422} 炒面.jpg \r\n",
      "\r\n",
      "{'莲藕排骨汤': 0.4674493074417114, '莲藕': 0.3581307828426361, '冬瓜汤': 0.009066015481948853} 莲藕排骨汤.jpg \r\n",
      "\r\n",
      "{'冰糖雪梨': 0.9577760696411133, '银耳汤': 0.0034847038332372904, '清炒山药': 0.001095216372050345} 冰糖雪梨.jpg \r\n",
      "\r\n",
      "{'鱼香肉丝': 0.930090606212616, '木耳炒肉丝': 0.010059439577162266, '莴笋肉丝': 0.0028328446205705404} 鱼香肉丝.jpg \r\n",
      "\r\n",
      "{'大米粥': 0.5422576069831848, '西湖牛肉羹': 0.15969528257846832, '皮蛋瘦肉粥': 0.09777914732694626} 西湖牛肉羹.jpg \r\n",
      "\r\n",
      "{'鱼香茄子': 0.8916314840316772, '肉末茄子': 0.038121841847896576, '蒜泥茄子': 0.005707074888050556} 鱼香茄子.jpg \r\n",
      "\r\n",
      "{'大米粥': 0.39374715089797974, '红薯粥': 0.17208372056484222, '小米粥': 0.10852274298667908} 大米粥.jpg \r\n",
      "\r\n",
      "{'红薯粥': 0.6780503988265991, '大米粥': 0.2483544945716858, '小米粥': 0.019263584166765213} 红薯粥.jpg \r\n",
      "\r\n",
      "{'水煮鱼': 0.28039926290512085, '毛血旺': 0.19879266619682312, '皮皮虾': 0.07771693915128708} 皮皮虾.jpg \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "path=r\"/home/aistudio/work/release_data/hi\"\n",
    "top_3s=predict_list(path,test_augs,model,mydicts,top_x=3)\n",
    "for i in range(len(top_3s[1])):\n",
    "    print(top_3s[0][i],top_3s[1][i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单个文件预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T13:02:43.934506Z",
     "iopub.status.busy": "2023-01-11T13:02:43.933459Z",
     "iopub.status.idle": "2023-01-11T13:02:44.651131Z",
     "shell.execute_reply": "2023-01-11T13:02:44.650106Z",
     "shell.execute_reply.started": "2023-01-11T13:02:43.934450Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'油焖大虾': 0.834388256072998, '香辣虾': 0.03609723225235939, '包子': 0.0033816404175013304, '肉酱意大利面': 0.0026113640051335096, '皮皮虾': 0.002368600806221366}\r\n"
     ]
    }
   ],
   "source": [
    "path=r\"/home/aistudio/work/release_data/hi/虾.jpg\"\r\n",
    "top_5s=predict(path, test_augs,model,mydicts, top_x=5)\r\n",
    "print(top_5s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10：自己玩"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
